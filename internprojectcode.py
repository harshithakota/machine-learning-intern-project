# -*- coding: utf-8 -*-
"""ML-MAJOR-APR-ML041B7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/129oVPArBiF-wJCYRmMQfFaqAWlyRMbOI

# **Loading the pertinent libraries**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pandas import get_dummies
from google.colab import files
import io
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import TruncatedSVD
from sklearn.decomposition import PCA
from scipy import stats
from google.colab import drive
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasRegressor
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import r2_score
import keras
from sklearn.model_selection import train_test_split
from keras.regularizers import l2
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
import xgboost
import seaborn as sns
from statsmodels.graphics.tsaplots import plot_acf
from scipy.stats import probplot
from pandas.plotting import bootstrap_plot

"""# **Uploading the train and test data set**

We upload the files to google colaboratory
"""

uploaded = files.upload()
uploaded1 = files.upload()

dfTest = pd.read_excel(io.BytesIO(uploaded1['Data_Test.xlsx'])) #Loading the test dataset
dfTrain = pd.read_excel(io.BytesIO(uploaded['Data_Train.xlsx'])) #Loading the train dataset

"""We drop the nan values from both the train and test data set provided. For cleaning the data and feature extraction, we join the train and test datasets. Also, in the price column we include a high value, so that the sets can be separated later when needed."""

dfTest.dropna(inplace = True)
dfTrain.dropna(inplace = True)
dfTest['Price'] = 1e8
dfTest.isnull().any(), dfTrain.isnull().any()

"""# **Data cleaning**

We join the test and train sets provided to us, for reasons specified before
"""

frames = [dfTrain, dfTest]
dfCombined = pd.concat(frames)
dfCombined.head()

dfCombined.tail()

"""We must keep the indices intact, so we create a separate column for the indices.This is done because, while processing the data, the identity of an individual entry would be lost if the indices aren't preserved, thus rendering the results we obtain by prediction useless."""

dfCombined.reset_index(inplace = True)
dfCombined.tail()

"""We note that many of these columns having numerical data have a unit associated with it, so we eliminate it. Also, some of the elements in 'Power' column had a 'null' value stored, since that indicates some lapse while collecting data, it would need to be removed."""

dfCombined['Mileage'] = dfCombined['Mileage'].apply(lambda x: float(list(x.split(' '))[0]))
dfCombined['Engine'] = dfCombined['Engine'].apply(lambda x: float(list(x.split(' '))[0]))  #Cleaning the data a little bit
dfCombined['Power'] = dfCombined['Power'].apply(lambda x: list(x.split(' '))[0]) #from experience, power has a 'null' string entry
lisNull = [index for index, element in enumerate(dfCombined['Power']) if element == 'null']
dfCombined.drop(lisNull, inplace = True)
dfCombined['Power'] = dfCombined['Power'].astype('float64')

"""Checking if any null values remain. We must keep checking for null values repeatedly to ensure that while processing, we don't lose any important information from the data."""

dfCombined.isnull().any()

"""# **Exploratory data analysis**

**Analysis of categorical variables**

Note that all analysis is done over the training dataset only. Deriving intuition from the structure of the test data doesn't make sense, as it is something that is available only when the model has been built and deployed.
"""

dfCombined.head()

dfToAnalyze = dfCombined[dfCombined['Price']!=1e8]
dfToAnalyze.head()

location = dfToAnalyze['Location'].value_counts()
print(location)
location.plot.pie()                                 #Most cars come from Mumbai and Hyderabad

year = dfToAnalyze['Year'].value_counts()
print(year)
year.plot.pie()                      #2014, 22015, 2016 and 2013 shoe the highest numbers of cars dating from thence

fuelType = dfToAnalyze['Fuel_Type'].value_counts()     #While LPG and petrol are almost the same, LPG having a marginal lead, CNG driven cars are quite low in proportion
print(fuelType)
fuelType.plot.pie()

Transmission = dfToAnalyze['Transmission'].value_counts()   #Manual transmission cars are more in number than Automatic transmission cars
print(Transmission)
Transmission.plot.pie()

ownerType = dfToAnalyze['Owner_Type'].value_counts()              #Second hand cars are larger in number than Third hand, and above. 
print(ownerType)
ownerType.plot.pie()

"""**Interrelation between different categorical variables**

Analysis of how different categorical variables interact can provide additional insights into the data, and how they are related.

We shall be analysing the relation between the following variables


1.   The dependence of Transmission type, Fuel, and Ownership type over Cities
2.   The dependence of aforementioned variables on the year the car was manufactured
"""

a = []
for i in dfToAnalyze['Location'].unique():
    a.append(dfToAnalyze[dfToAnalyze.Location == i].Transmission.value_counts())
    
a = pd.DataFrame(a)
a.index = dfToAnalyze['Location'].unique()
a.plot.bar()

"""At almost every location manual type transmission vehicles are more than automatic type transmission vehicles. The disparity though, is highest in Kolkata and the least in Bangalore

"""

a = []
for i in dfToAnalyze['Location'].unique():
    a.append(dfToAnalyze[dfToAnalyze.Location == i].Fuel_Type.value_counts())
    
a = pd.DataFrame(a)
a.index = dfToAnalyze['Location'].unique()
a.plot.bar()

"""The usage of petrol is higher in Mumbai, Pune, Kochi, and Kolkata among the second hand vehicles.In other cities though, the usage of Diesel is higher"""

a = []
for i in dfToAnalyze['Location'].unique():
    a.append(dfToAnalyze[dfToAnalyze.Location == i].Owner_Type.value_counts())
    
a = pd.DataFrame(a)
a.index = dfToAnalyze['Location'].unique()
a.plot.bar()

"""First hand vehicles are more abundant in all cities, and show no specific variation with cities"""

a = []
for i in dfToAnalyze['Year'].unique():
    a.append(dfToAnalyze[dfToAnalyze.Year == i].Transmission.value_counts())
    
a = pd.DataFrame(a)
a.index = dfToAnalyze['Year'].unique()
a.plot.bar()

"""The recent years show a huge surge in the number of vehicles on sale from that year of manufacture, which isn't surprising. The surge has occurred mostly beyond 2009"""

a = []
for i in dfToAnalyze['Year'].unique():
    a.append(dfToAnalyze[dfToAnalyze.Year == i].Fuel_Type.value_counts())
    
a = pd.DataFrame(a)
a.index = dfToAnalyze['Year'].unique()
a.plot.bar()

"""The cars manufactured beyond year 2010 show an increase in the usage of Diesel as a fuel, as compared to Petrol for the cars manufactured in previous years.

"""

a = []
for i in dfToAnalyze['Year'].unique():
    a.append(dfToAnalyze[dfToAnalyze.Year == i].Owner_Type.value_counts())
    
a = pd.DataFrame(a)
a.index = dfToAnalyze['Year'].unique()
a.plot.bar()

"""Second hand cars are always more popular and in sale than the cars that have been used more than twice or more times. Also, beyond 2010, there is a sudden surge in the number of cars available that were manufactured during that period, which can be imputed to technological advancements.

**Univaraite analysis of continuous variables**

Before applying any model, it is necessary to check whether the continuous variables follow the assumptions of univariate model y = central tendency + error
where the error is random, vraiance remains same throughout, from a fixed distribution and with no correlation within the data. These assumptions would validate (or invalidate) the assumptions we make in future while building the models.
"""

kilometersDriven = dfToAnalyze['Kilometers_Driven']
mileage = dfToAnalyze['Mileage']
engineVolume = dfToAnalyze['Engine']
power = dfToAnalyze['Power']
price = dfToAnalyze['Price']

# 1. Run Sequence plot, to check if the assumptions of univariate uncorrelated data remain valid

plt.plot(kilometersDriven, color = Blue)

plt.plot(mileage, color = 'red')

plt.plot(engineVolume, color = 'orange')

plt.plot(power, color = 'green')

plt.plot(price, color = 'purple')

"""The data shows random variation about an unchanging mean throughout the span. The variation also remains nearly constant, with slight fluctuations. Also, the data shows presence of several outliers that need to be eliminated, before the data is used for training any model"""

# 2. Lag plot for checking correlation in data, if present. We plot yi as a function of yi-1

plt.scatter(np.insert(np.array(kilometersDriven)[:-1], 0, np.array(kilometersDriven)[-1], axis = 0), kilometersDriven)

plt.scatter(np.insert(np.array(mileage)[:-1], 0, np.array(mileage)[-1], axis = 0), mileage, color = 'red')

plt.scatter(np.insert(np.array(engineVolume)[:-1], 0, np.array(engineVolume)[-1], axis = 0), engineVolume, color = 'orange')

plt.scatter(np.insert(np.array(power)[:-1], 0, np.array(power)[-1], axis = 0), power, color = 'green')

plt.scatter(np.insert(np.array(price)[:-1], 0, np.array(price)[-1], axis = 0), price, color = 'purple')

"""The data is clearly random, as it doesn't form a specified shape of structure. Thus there should be negligible autocorrelation in the data, which we check using Autocorrelation plots in the next section."""

# 3. Autocorrelation plots, for checking if the data has any correlation, which should be absent as is evident from the previous analysis

plot_acf(kilometersDriven, lags = 50)

plot_acf(mileage, lags = 50)

plot_acf(engineVolume, lags = 50)

plot_acf(power, lags = 50)

plot_acf(price, lags = 50)

"""Negligible autocorrelation even beyond the first step, as we had expected"""

# 4. Histograms, getting a sense of the univariate distribution

sns.set(color_codes = True)

sns.distplot(kilometersDriven, bins = 20)

sns.distplot(mileage, bins = 20)

sns.distplot(engineVolume, bins = 20)

sns.distplot(power, bins = 20)

sns.distplot(price, bins = 20)

"""The distributions significantly deviate from the ideal normal distribution. There are some with a more number of peaks, small vraiation around the mean, and presence of tails around the data. The presence of tails is another reason to convince us of the presence of outliers which we should remove so that the model works best on the data.

"""

# 4. Normal probability plot. To see whether a normal distribution explains the trends in the data, and if so, how well.

fig = plt.figure()
ax = fig.add_subplot(1,1,1)
probplot(kilometersDriven, plot = ax)
plt.show()

fig = plt.figure()
ax = fig.add_subplot(1,1,1)
probplot(mileage, plot = ax)
plt.show()

fig = plt.figure()
ax = fig.add_subplot(1,1,1)
probplot(engineVolume, plot = ax)
plt.show()

fig = plt.figure()
ax = fig.add_subplot(1,1,1)
probplot(power, plot = ax)
plt.show()

fig = plt.figure()
ax = fig.add_subplot(1,1,1)
probplot(price, plot = ax)
plt.show()

"""There is a significant deviation from the normal distribution. This points to several complexities in the data that might not be captured by simple linear models and directs us to try more complex models for explaining the variation in the data"""

# 5. Uncertainty of statistics, median mean and midrange plots: Bootstrap plot

bootstrap_plot(kilometersDriven, samples = 500, color = 'blue')          #The median shows the greatest degree of variation, so it is the best estimate of central tendency

bootstrap_plot(mileage, samples = 500, color = 'red')   #The median and mean in this case capture the most variation

bootstrap_plot(engineVolume, samples = 500, color = 'orange') # The mean captures most of the variation of the data

bootstrap_plot(power, samples = 500, color = 'green')  #The mean again, captures most of the variation in the data

bootstrap_plot(price, samples = 500, color = 'purple')  # The mean in this case too, covers most of the variation in the data

"""The above plots show that there is no generalised estimate of the central tendency for the columns, which directs us to use more complex models while training the data.

**Outlier detection**

We shall now try to detect outliers in the continuous variables.
"""

sns.boxplot(kilometersDriven,color='blue')               #Several outliers present, one too far off the mean.

sns.boxplot(mileage,color='red')                   #Fewers outliers present, on both the ends, still present though, and need to be eliminated

sns.boxplot(engineVolume,color='orange')                      # Several outliers present over one end of the data, basically presence of

sns.boxplot(power,color='green')               #Several outliers present on one side, as in above, so they better be eliminated.

sns.boxplot(price,color='purple')        #Several outliers present, which should be eliminated, mostly on one end of the data

"""Several ouliers were detected in the continous variables which we shall remove for the purpose of getting a better fit in the model

**Dependence of target on different continuous variables**

We have four target variables, Kilometeres driven, engine, power and mileage. It would be interesting to check to what extent they depend on the continuous feature variables.
"""

sns.jointplot("Kilometers_Driven", "Price", data=dfToAnalyze, kind='reg',color='blue')

"""Highly non-linear curve, data points are segregated on one side of the range, and thus a large outlier is present."""

sns.jointplot("Mileage", "Price", data=dfToAnalyze, kind='reg',color='red')

"""The price is shown to decrease with an increase in mileage, which is a highly counterintuitive result. The use of a linear model can't capture most of the trends in data, and as we shall see later, linear models don't perform very well on the given data"""

sns.jointplot("Engine", "Price", data=dfToAnalyze, kind='reg',color='orange')

"""With an increase in engine volume, the price of the car goes up, which is expected. Although, there are several outliers and much of the variation in the data isn't accounted for by a single linear model"""

sns.jointplot("Power", "Price", data=dfToAnalyze, kind='reg',color='green')

"""The price goes up with the power of the vehicle, which is again expected. As before, there are several ouliers and data has a big variation about the linear model.

**Multivariate analysis**

Now we shall simply observe the variation of price across different continuous variables, and categorical variables to understand the underlying structure of the data.
"""

plt.figure(figsize = (15,5))
sns.scatterplot(x = 'Engine', y = 'Power', hue = 'Fuel_Type', 
           data = dfToAnalyze)
plt.title('Engine and Power with Fuel Type')

"""The power delivered by the engine increases with an increase in the engine capacity. This is a linear trend, and thus the price and power columns are expected to have a high corelation. While petrol automobiles are spread over the expanse of the power-engine capacity range, diesel autombiles are located towards the lower end of  power-engine capacity range. The rest might be treated as outliers."""

plt.figure(figsize = (15,5))
sns.scatterplot(x = 'Engine', y = 'Mileage', hue = 'Fuel_Type', 
           data = dfToAnalyze)
plt.title('Engine and Mileage with Fuel Type')

"""The mileage reduces with an increase in engine volume as a general trend, although a lot of variation is seen towards the lower end of the expanse. Irrespective of fuel type, the density of instances is higher towards the lower end of the mileage-engine capacity spectrum"""

plt.figure(figsize = (15,5))
sns.scatterplot(x = 'Power', y = 'Mileage', hue = 'Fuel_Type', 
           data = dfToAnalyze)
plt.title('Power and Mileage with Fuel Type')

"""The mileage provided by the vehicle goes down slightly as the power increases. All fuel kinds are densely scattered towards the left side of the expanse, which suggests the presence of outliers."""

plt.figure(figsize = (15,5))
sns.scatterplot(x = 'Engine', y = 'Power', hue = 'Year', 
           data = dfToAnalyze)
plt.title('Engine and Power with Year')

"""The power of the vehicle has increased with an increase in engine volume, which isn't unexpected. These show a high corelation. The desnity increases with increase in the manufacturing year, which indicates improvement in automobile manufacture over the years."""

plt.figure(figsize = (15,5))
sns.scatterplot(x = 'Engine', y = 'Mileage', hue = 'Year', 
           data = dfToAnalyze)
plt.title('Engine and Mileage with Year')

"""The same trend as was discussed above is followed. Newly manufatured automobiles are concentrated towards the left end of the spectrum, which corresponds to increase in mileage of the vehicle with year."""

plt.figure(figsize = (15,5))
sns.scatterplot(x = 'Power', y = 'Mileage', hue = 'Year', 
           data = dfToAnalyze)
plt.title('Power and Mileage with Year')

"""The variation is the same as noted in above plots"""

plt.figure(figsize = (15,5))
sns.scatterplot(x = 'Engine', y = 'Power', hue = 'Transmission', 
           data = dfToAnalyze)
plt.title('Engine and Power with Transmission')

"""The variation is the same as noted in a similar figure previously. Although, automatic transmission engines have a higher spread and are located higher in the plot."""

plt.figure(figsize = (15,5))
sns.scatterplot(x = 'Engine', y = 'Mileage', hue = 'Transmission', 
           data = dfToAnalyze)
plt.title('Engine and Mileage with Transmission')

"""The variation is similar as desribed in the previous few figures"""

plt.figure(figsize = (15,5))
sns.scatterplot(x = 'Power', y = 'Mileage', hue = 'Transmission', 
           data = dfToAnalyze)
plt.title('Power and Mileage with Transmission')

"""The variation in quantities is same as noted in previous few figures."""

plt.figure(figsize = (15,5))
sns.scatterplot(x = 'Engine', y = 'Power', hue = 'Owner_Type', 
           data = dfToAnalyze)
plt.title('Engine and Power with Owner_Type')

"""The variation in power-engine capacity is the same as described before. The second hand vehicles show a higher spread which decreases with increase in ownership of the vehicle."""

plt.figure(figsize = (15,5))
sns.scatterplot(x = 'Engine', y = 'Mileage', hue = 'Owner_Type', 
           data = dfToAnalyze)
plt.title('Engine and Mileage with Owner_Type')

"""The variation with the three variables is similar as has been described in the previous few plots."""

plt.figure(figsize = (15,5))
sns.scatterplot(x = 'Power', y = 'Mileage', hue = 'Owner_Type', 
           data = dfToAnalyze)
plt.title('Power and Mileage with Owner_Type')

"""The variation in the three variables goes the same way as has been described in the previous few figures.

**Variation of feature with categorical variables**
"""

fig = plt.figure()
plt.subplot(1,3,1)
plt.scatter(dfToAnalyze['Price'],dfToAnalyze['Location'],c='blue')
plt.xlabel('Price(in Lakhs)')
plt.ylabel('Location')

"""Highest variation of price is seen in Coimbatore (ignoring outliers), and the least variation in automobile prices is seen in Jaipur"""

fig = plt.figure()
plt.subplot(1,3,1)
plt.scatter(dfToAnalyze['Price'],dfToAnalyze['Year'],c='red')
plt.xlabel('Price(in Lakhs)')
plt.ylabel('Year')
plt.subplot(1,3,3)
plt.plot(dfToAnalyze.groupby('Year').Price.mean())
plt.ylabel('Mean of the prices with year')
plt.xlabel('Year')

"""The variation in prices of the autombiles increase with the year of manufacture. Also, the average prices are seen to increase with each year too."""

fig = plt.figure()
plt.subplot(1,3,1)
plt.scatter(dfToAnalyze['Price'],dfToAnalyze['Fuel_Type'],c='orange')
plt.xlabel('Price(in Lakhs)')
plt.ylabel('Fuel Type')
plt.subplot(1,3,3)
plt.plot(dfToAnalyze.groupby('Fuel_Type').Price.mean())
plt.ylabel('Mean of the prices with fuel type')
plt.xlabel('Fuel Type')

"""Petrol and Diesel have the highest variation in the automobile price, as well as in the average prices. The contrary is valid for the other two."""

fig = plt.figure()
plt.subplot(1,3,1)
plt.scatter(dfToAnalyze['Price'],dfToAnalyze['Transmission'],c='green')
plt.xlabel('Price(in Lakhs)')
plt.ylabel('Transmission')
plt.subplot(1,3,3)
plt.plot(dfToAnalyze.groupby('Transmission').Price.mean())
plt.ylabel('Mean of the prices with transmission type')
plt.xlabel('Transmission type')

"""Autoamtic transmission automobiles have higher variation in the prices as well as higher mean prices.

# **Data cleaning and feature extraction**

The column 'Name' contains test data which might be used in improving our model. We would need to process the data to render it useful for the model, all exploratory data analysis has been done before, and from this point on, we would focus solely on feature extraction to improve the performance of the models.

The first step required us to vectorise available data
"""

textData = dfCombined['Name']
vectoriser = CountVectorizer()
textData = vectoriser.fit_transform(textData).toarray()
textData.shape

"""Now the no of columns in 'Name' is very high. That would unnecessarily increase the amount of data and computational capacity required for processing it and training the model over it. We would like to keep the no of features as low as possible and capture most of the variance in the data. Using principal component analysis, we can get an idea of how many columns would capture around 0.98 of the vatiation in the data. Then we would Truncated Singualar Value decomposition, a popular transform similar to PCA, applied for sparse matrices because of imaginary eigen values arising in case PCA is applied. It returns a matrix of the columns that capture 0.98 of the variation in the data"""

principalComponent = PCA(0.98)    #We can vary the parameter later, let's see how this works for the time bein
textDataModified = principalComponent.fit_transform(textData)
textDataModified.shape, textData.shape, principalComponent.n_components_

svDec = TruncatedSVD(n_components=principalComponent.n_components_, n_iter=10)

textDataTransformed = svDec.fit_transform(textData)
textDataTransformed.shape

# dfCombined.drop(['Name'], axis = 1, inplace = True)
textFeatures = pd.DataFrame(textDataTransformed)
textFeatures.isnull().any(), textFeatures.shape


textFeatures.tail()

dfCombined.shape, textFeatures.isnull().any(), textFeatures.shape, dfCombined.isnull().any()

"""The textFeatures matrix returned by TruncatedSVD is appended as a feature to the dataframe, while the name column (which it was derived from) is simply dropped out.


"""

dfCombined.reset_index(drop = True, inplace = True)
textFeatures.reset_index(drop = True, inplace = True)
dfCombinedModified = pd.concat([dfCombined, textFeatures], axis = 1)
dfCombinedModified.tail()

dfCombinedModified.isnull().any()

"""We do one hot encoding for all the categorical variables so that they can be fed into the model."""

mEnc = pd.get_dummies(dfCombinedModified, columns = ['Location', 'Fuel_Type', 'Transmission', 'Owner_Type'])

mEnc.head()

mEnc = mEnc.drop(['Name'], axis = 1)

mEnc.shape
mEnc.head()

"""Exploratory data analysis revealed the presence of several outliers in the continuous variables, we can remove them from the training data. This would improve the performance of whatever model we would be applying over the data. Also, note that this isn't done over the test data, because that would invariably mean not performing prediction on the model even though all the features are present."""

rEncTest = mEnc[mEnc['Price']==1e8]
rEncTrain = mEnc[mEnc['Price']!= 1e8]
rEncTrain.shape, rEncTest.shape  #Note that the columns having a null value 'Power' column have been removed from the test set

rEncTrain.head()

rEncTrain.drop(['index'], axis = 1, inplace = True)
rEncTrain.head()

rEncTest.head()

"""The Z score tells us how much a feature is away from the mean of the dataset. A Z score of 3 for instance, would represent that the datapoint is 3 standard deviations away from the mean of the data. We ignore the entries with a Z score greater than 3 as outliers in the data."""

zScore = np.abs(stats.zscore(rEncTrain['Kilometers_Driven']))
R = rEncTrain[zScore < 3]
# R.info()
zScore = np.abs(stats.zscore(R['Mileage']))
R = R[zScore < 3]
# R.info()
zScore = np.abs(stats.zscore(R['Engine']))
R = R[zScore < 3]
# R.info()
zScore = np.abs(stats.zscore(R['Power']))
R = R[zScore < 3]
zScore = np.abs(stats.zscore(R['Price']))
R = R[zScore < 3]
R.info()

R.head()

R.reset_index(inplace = True, drop = True)
R.head()

y = R.iloc[:, 6]
X = R.drop(['Price'], axis = 1)
X.shape, y.shape

#Normalising the columns and storing the value in a list, for normalising the test set
X.head()

"""For training a model over some dataset, it really speeds up the training process if one normalises the data. We normalise the data and _leak_ the *mean* and *standard deviation* for the columns, to be used for normalising the test data before using the model to make predictions over it"""

def featureScaling(dataFrame):
  cache = []
  for column in dataFrame:
    cache.append([np.mean(dataFrame[column]), np.std(dataFrame[column])])
    dataFrame[column] = (dataFrame[column] - np.mean(dataFrame[column]))/np.std(dataFrame[column])
  return cache

cache = featureScaling(X) #Storing the values

rEncTest.head()

"""The test feature variables are normalised using the _cache_ _leaked_ from the train data set. Also, the index column is redundant, so it's dropped, and can be used to identify a given test entry while making predictions using the model."""

def testFeatureScaling(dataFrame, cache):
  dataFrame = np.array(dataFrame)
  dataFrame = (dataFrame - np.array(cache).T[0, :])/(np.array(cache).T[1, :])
  return pd.DataFrame(dataFrame)

index = rEncTest['index']
rEncTest.drop(['index', 'Price'], axis = 1, inplace = True)

np.array(cache).shape, rEncTest.shape

testDataCleaned = testFeatureScaling(rEncTest, cache)
testDataCleaned.head()

rEncTest.head()

"""We check for null values, in the training data set (X,y) and proceed to build and train our model."""

X.isnull().any(), testDataCleaned.isnull().any(), y.isnull().any()

"""## **Model selection**

Splitting the data into an appropriate ratio of 3:1 for train:test
"""

XTrain, XTest, yTrain, yTest = train_test_split(X, y, test_size = 0.25)
XTrain.shape, XTest.shape, yTrain.shape, yTest.shape

"""**Linear regression model**

The trends as we concluded can't be explained well based on a simple linear model. But it would be a good point to observe its performance on the given dataset, and improve over it successively with better models.
"""

#Making the linear regression model
model2 = LinearRegression()
model2.fit(XTrain, yTrain)
yPred = model2.predict(XTest)
r2_score(yTest, yPred)

"""**Polynomial regression**

The linear model didn't perform really well on the given dataset. So, we transform the feature variables into  fourth degree polynomial features, and then fit a linear model to it, in short, do polynomiual regression.
"""

# polyReg = PolynomialFeatures(degree = 3)
# XPoly = polyReg.fit_transform(XTrain)
# model3 = LinearRegression()
# model3.fit(XPoly, yTrain)
# yPred = model3.predict(polyReg.transform(XTest))
# r2_score(yTest, yPred)

"""Given the high dimensionality of the data, it's difficlut to train a polynomial model over the given data, the runtime crashes due to exceeding required RAM. So we try other approaches.

**Decision Tree regressor**

We use a decision tree having a large number of estimators to fit the given data, and observe its performance over the given dataset.
"""

model4 = DecisionTreeRegressor(random_state = 0)
model4.fit(XTrain, yTrain)
yPred = model4.predict(XTest)
r2_score(yTest, yPred)

"""**Random forest regression**

We apply random forest regression, an ensemble learning technique that 
"""

# model5 = RandomForestRegressor(n_estimators = 500, random_state = 0)
# model5.fit(XTrain, yTrain)
# yPred = model5.predict(XTest)
# r2_score(yTest, yTred)

"""Random forest regression suffers the same problem as polynomial regression models. Due to high dimensionality of data, we fail to train the model over an ensemble of decision trees.

**The deep neural network model**

We do hyperparameter tuning and settle over the no of layers and no. of nodes in each layer, which reduces the mean squared error over the training dataset. We use a validation ratio of 0.1 for testing the validation accuracy and its accuracy over the course of training. With appropriate tuning we end up with a model that performs well.
"""

model1 = Sequential()
model1.add(Dense(512, input_dim = XTrain.shape[1], kernel_initializer='normal', activation = 'relu', kernel_regularizer=l2(0.01)))
model1.add(Dense(256, kernel_initializer='normal', activation = 'relu', kernel_regularizer=l2(0.01)))
model1.add(Dense(256, kernel_initializer='normal', activation = 'relu', kernel_regularizer=l2(0.01)))
model1.add(Dense(256, kernel_initializer = 'normal', activation = 'relu', kernel_regularizer=l2(0.01)))
model1.add(Dense(1, kernel_initializer='normal', activation='linear'))
opt = keras.optimizers.Adam(learning_rate = 5e-4)
model1.compile(loss = 'mean_squared_error', optimizer = opt, metrics = ['mean_squared_error', 'mean_absolute_error'])
model1.summary()
history = model1.fit(XTrain, yTrain, epochs= 400 , batch_size= 256 ,  verbose=1, validation_split=0.1)

yPred = model1.predict(XTrain)
yPredTest = model1.predict(XTest)
r2_score(yTrain, yPred), r2_score(yTest, yPredTest)

"""A lot of the models couldn't be trained due to the size of the data and RAM limitations. Those have been commented out.

# **Predicting for the test dataset**

For identifying each entry of the test data set, post the dropping of null values, we stored them in an index variable, so we shall load it and concatenate it with the prediction for the model, and present the results. The neural network model provided the best r_2 score among others, so we shall deploy it for the purpose of this project.
"""

index.shape, rEncTest.shape

predictedPrices = model1.predict(rEncTest)

predictedPrices = np.array(predictedPrices)
predictedPrices.shape

index = np.array(index)
index
index = index.reshape((1201,1))

output = np.concatenate([index, predictedPrices])       #The output for the model

np.min(predictedPrices)

np.max(predictedPrices)

plt.plot(predictedPrices)

